{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# WI4450 - Special Topics in Computational Science and Engineering\n",
    "# Alexander Heinlein\n",
    "#\n",
    "# Decomposing Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lecture, we will discuss approaches for **decomposing neural networks**. In the context of neural networks, there are different reasons to decompose neural networks. We will focus on:\n",
    "\n",
    "+ **Parallelization and efficiency** <br> and\n",
    "+ **Robustness and convergence**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, there are approaches which take a **given neural network and decompose the application of that given network** and approaches which **develop a decomposed neural network archiecture** itself. We will briefly discuss the first approach and then mostly focus on the second approach and link it to approaches from numerical analysis and scientific computing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parallelization Concepts for Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, let us consider the some of the parallelization concepts discussed in:\n",
    "\n",
    "> Fedus, William, Barret Zoph, and Noam Shazeer. \"Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity.\" Journal of Machine Learning Research 23.120 (2022): 1-39.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "The following graphic is taken from that article and shows the **different concepts for parallelization of neural networks**:\n",
    "\n",
    "<img src=\"fig/parallelization.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "We can see several data weight and partitioning strategies, the first three of which we will discuss:\n",
    "\n",
    "+ **Data parallelism**\n",
    "+ **Model parallelism**\n",
    "+ **Model and data parallelism**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data Parallelism\n",
    "\n",
    "In the data parallelism approach, the **data set is split into different parts** and the same model is applied to each part. This is the **most common and most simple approach** for parallelizing neural networks. \n",
    "+ In **distributed-memory contexts**, this can be implemented using different devices; this goes beyond the scope of this lecture. \n",
    "+ In **shared-memory contexts**, this can be implemented using threads. Using Jax, this can easily be implemented using the `vmap` function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This means that we do not parallelize the computation of a single application of the model but parallelize the sum in the **mean squared error (MSE) loss function**\n",
    "$$\n",
    "    \\min_{\\theta} \\underbrace{\\frac{1}{n} \\sum_{i=1}^{n} \\left( \\mathcal{N}_\\theta (x_i,t_i) - y_i \\right)^2}_{=: \\mathcal{L}(\\theta)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In particular, we split the $n$ indices into $P$ parts $I_1, \\ldots, I_P$ and compute the sum as\n",
    "$$\n",
    "    \\mathcal{L}(\\theta) = \\sum_{p=1}^{P} \\underbrace{\\sum_{i \\in I_p} \\left( \\mathcal{N}_\\theta (x_i,t_i) - y_i \\right)^2}_{\\mathcal{L}_p(\\theta)},\n",
    "$$\n",
    "where each part can be done completely in parallel:\n",
    "$$\n",
    "    \\sum_{i \\in I_p} \\left( \\mathcal{N}_\\theta (x_i,t_i) - y_i \\right)^2\n",
    "$$\n",
    "\n",
    "Only for computing the final sum, we need to **synchronize/communicate**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Then, in each step of **gradient descent**,\n",
    "$$\n",
    "    \\theta_{k+1} = \\theta_k - \\alpha \\nabla \\mathcal{L}(\\theta_k),\n",
    "$$\n",
    "we also have to compute the gradient of the loss function $\\mathcal{L}(\\theta)$ in parallel, which is simple due to the linearity of the gradient:\n",
    "$$\n",
    "    \\nabla \\mathcal{L}(\\theta) = \\sum_{p=1}^{P} \\nabla \\mathcal{L}_p(\\theta).\n",
    "$$\n",
    "\n",
    "Then, the gradients computed in parallel have to be summed up to update the parameters $\\theta$. In shared-memory contexts, this again requires synchronization, but in distributed-memory contexts, this requires the **communcation of the gradients between the different devices**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will now investigate this using a Python example using Jax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we consider again our the `FeedForwardNN` class from the previous lecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import random, vmap, jit\n",
    "import jax.numpy as jnp\n",
    "\n",
    "class FeedForwardNN:\n",
    "    def __init__(self, layer_sizes, key, activation_fn=jax.nn.tanh):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.activation_fn = activation_fn\n",
    "        self.params = self.initialize_params(layer_sizes, key)\n",
    "\n",
    "    def initialize_params(self, layer_sizes, key):\n",
    "        params = []\n",
    "        keys = random.split(key, len(layer_sizes) - 1)\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            W_key, b_key = random.split(keys[i])\n",
    "            # Initialize weights with normal distribution and scale\n",
    "            W = random.uniform(W_key, (layer_sizes[i], layer_sizes[i+1]), minval=-1.0, maxval=1.0)\n",
    "            # Initialize biases with zeros\n",
    "            b = jnp.zeros(layer_sizes[i+1])\n",
    "            params.append((W, b))\n",
    "        return params\n",
    "\n",
    "    def forward(self, params, x):\n",
    "        for W, b in params[:-1]:\n",
    "            # Linear transformation\n",
    "            x = jnp.dot(x, W) + b\n",
    "            # Apply activation function\n",
    "            x = self.activation_fn(x)\n",
    "        # Output layer (no activation function)\n",
    "        W, b = params[-1]\n",
    "        x = jnp.dot(x, W) + b\n",
    "        return x\n",
    "\n",
    "    def predict(self, x):\n",
    "        # Predict output for input x\n",
    "        return vmap(self.forward, in_axes=(None, 0))(self.params, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now, we will train this network to fit a simple $\\sin$ function using many data points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate sample data\n",
    "x = jnp.linspace(-1.0, 1.0, 1000)\n",
    "y = jnp.sin(jnp.pi*x)\n",
    "\n",
    "# Plot the results\n",
    "plt.plot(x, y, label='Sine function')\n",
    "plt.gcf().set_size_inches(4, 2) \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let us first implement a non-parallelized version of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "from tqdm import trange\n",
    "\n",
    "# Define the neural network\n",
    "key = random.PRNGKey(0)\n",
    "layer_sizes = [1, 20, 1] \n",
    "nn = FeedForwardNN(layer_sizes, key)\n",
    "\n",
    "# Create a learning rate schedule\n",
    "learning_rate_schedule = optax.piecewise_constant_schedule(\n",
    "    init_value=0.1,\n",
    ")\n",
    "\n",
    "# Initialize the Adam optimizer with the learning rate schedule\n",
    "optimizer = optax.adam(learning_rate=learning_rate_schedule)\n",
    "opt_state = optimizer.init(nn.params)\n",
    "\n",
    "# Define the loss function (Mean Squared Error)\n",
    "squared_error = lambda params, x, y: (nn.forward(params, x) - y) ** 2\n",
    "def loss_fn(params, x, y):\n",
    "    squared_errors = jnp.array([squared_error(params, xi, yi) for xi, yi in zip(x, y)])\n",
    "    return jnp.mean(squared_errors)\n",
    "\n",
    "# Define training step with Adam optimizer\n",
    "def train_step(params, opt_state, x, y):    \n",
    "    loss, grads = jax.value_and_grad(loss_fn)(params, x, y)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return new_params, opt_state, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now, we train the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "losses = []\n",
    "max_iterations = 10\n",
    "\n",
    "# Training loop with tqdm progress bar\n",
    "pbar = trange(max_iterations, desc=\"Training\", leave=True)\n",
    "for epoch in pbar:\n",
    "    nn.params, opt_state, current_loss = train_step(nn.params, opt_state, x, y)\n",
    "    losses.append(current_loss)\n",
    "    pbar.set_postfix(loss=current_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is very slow. Finally, we plot the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with two subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 2))\n",
    "\n",
    "# Plot the loss over the number of iterations\n",
    "axs[0].set_yscale('log')\n",
    "axs[0].plot(range(max_iterations), losses, label='Loss')\n",
    "axs[0].set_xlabel('Iteration')\n",
    "axs[0].set_ylabel('Loss')\n",
    "axs[0].legend()\n",
    "\n",
    "# Plot the results\n",
    "axs[1].plot(x, y, label='Sine function')\n",
    "axs[1].plot(x, nn.predict(x).reshape(y.shape), label='Neural network')\n",
    "axs[1].legend()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The iteration is very slow because each data sample is processed **sequentially in a for loop**:\n",
    "\n",
    "```python\n",
    "squared_errors = jnp.array([squared_error(params, xi, yi) for xi, yi in zip(x, y)])\n",
    "```\n",
    "\n",
    "Now, we will use **data parallelism** to speed up the training. We now **parallelize over the data samples** using the `vmap` function from Jax:\n",
    "\n",
    "```python\n",
    "squared_errors = vmap(squared_error, in_axes=(None, 0, 0))(params, x, y)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import optax\n",
    "from jax import vmap\n",
    "from tqdm import trange\n",
    "\n",
    "# Define the neural network\n",
    "key = random.PRNGKey(0)\n",
    "layer_sizes = [1, 20, 1] \n",
    "nn = FeedForwardNN(layer_sizes, key)\n",
    "\n",
    "# Create a learning rate schedule\n",
    "learning_rate_schedule = optax.piecewise_constant_schedule(\n",
    "    init_value=0.1\n",
    ")\n",
    "\n",
    "# Initialize the Adam optimizer with the learning rate schedule\n",
    "optimizer = optax.adam(learning_rate=learning_rate_schedule)\n",
    "opt_state = optimizer.init(nn.params)\n",
    "\n",
    "# Define the loss function (Mean Squared Error)\n",
    "squared_error = lambda params, x, y: (nn.forward(params, x) - y) ** 2\n",
    "def loss_fn(params, x, y):\n",
    "    squared_errors = vmap(squared_error, in_axes=(None, 0, 0))(params, x, y)\n",
    "    return jnp.mean(squared_errors)\n",
    "\n",
    "# Define training step with Adam optimizer\n",
    "def train_step(params, opt_state, x, y):    \n",
    "    loss, grads = jax.value_and_grad(loss_fn)(params, x, y)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return new_params, opt_state, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We again train for the same number of epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "losses = []\n",
    "max_iterations = 10\n",
    "\n",
    "# Training loop with tqdm progress bar\n",
    "pbar = trange(max_iterations, desc=\"Training\", leave=True)\n",
    "for epoch in pbar:\n",
    "    nn.params, opt_state, current_loss = train_step(nn.params, opt_state, x, y)\n",
    "    losses.append(current_loss)\n",
    "    if epoch % 100 == 0:\n",
    "        pbar.set_postfix(loss=current_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And we plot to obtain the same result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with two subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 2))\n",
    "\n",
    "# Plot the loss over the number of iterations\n",
    "axs[0].set_yscale('log')\n",
    "axs[0].plot(range(max_iterations), losses, label='Loss')\n",
    "axs[0].set_xlabel('Iteration')\n",
    "axs[0].set_ylabel('Loss')\n",
    "axs[0].legend()\n",
    "\n",
    "# Plot the results\n",
    "axs[1].plot(x, y, label='Sine function')\n",
    "axs[1].plot(x, nn.predict(x).reshape(y.shape), label='Neural network')\n",
    "axs[1].legend()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We have obtained the same result, in a fraction of the time. We can easily **run the same training for many more epochs in short time**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "from jax import vmap\n",
    "from tqdm import trange\n",
    "\n",
    "# Define the neural network\n",
    "key = random.PRNGKey(0)\n",
    "layer_sizes = [1, 20, 1] \n",
    "nn = FeedForwardNN(layer_sizes, key)\n",
    "\n",
    "# Create a learning rate schedule\n",
    "learning_rate_schedule = optax.piecewise_constant_schedule(\n",
    "    init_value=0.1,\n",
    "    boundaries_and_scales={300: 0.5, 600: 0.5}\n",
    ")\n",
    "\n",
    "# Initialize the Adam optimizer with the learning rate schedule\n",
    "optimizer = optax.adam(learning_rate=learning_rate_schedule)\n",
    "opt_state = optimizer.init(nn.params)\n",
    "\n",
    "# Define the loss function (Mean Squared Error)\n",
    "squared_error = jit(lambda params, x, y: (nn.forward(params, x) - y) ** 2)\n",
    "@jit\n",
    "def loss_fn(params, x, y):\n",
    "    squared_errors = vmap(squared_error, in_axes=(None, 0, 0))(params, x, y)\n",
    "    return jnp.mean(squared_errors)\n",
    "\n",
    "# Define training step with Adam optimizer\n",
    "def train_step(params, opt_state, x, y):    \n",
    "    loss, grads = jax.value_and_grad(loss_fn)(params, x, y)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return new_params, opt_state, loss\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "max_iterations = 1000\n",
    "\n",
    "# Training loop with tqdm progress bar\n",
    "pbar = trange(max_iterations, desc=\"Training\", leave=True)\n",
    "for epoch in pbar:\n",
    "    nn.params, opt_state, current_loss = train_step(nn.params, opt_state, x, y)\n",
    "    losses.append(current_loss)\n",
    "    if epoch % 100 == 0:\n",
    "        pbar.set_postfix(loss=current_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now, we obatain a much better fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with two subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 2))\n",
    "\n",
    "# Plot the loss over the number of iterations\n",
    "axs[0].set_yscale('log')\n",
    "axs[0].plot(range(max_iterations), losses, label='Loss')\n",
    "axs[0].set_xlabel('Iteration')\n",
    "axs[0].set_ylabel('Loss')\n",
    "axs[0].legend()\n",
    "\n",
    "# Plot the results\n",
    "axs[1].plot(x, y, label='Sine function')\n",
    "axs[1].plot(x, nn.predict(x).reshape(y.shape), label='Neural network')\n",
    "axs[1].legend()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model Parallelism\n",
    "\n",
    "In the model parallelism approach, the **model itself is split into different parts** and each part is applied to the data. This can be useful if the **model is too large to fit on a single device**. The main disadvantage is that the model has to be split into different parts, which can lead to **communication overhead in the case of distributed-memory parallelism**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a simple feedforward neural network, parallelization of the model means **splitting up the weights and biases of the network** and then **parallelizing the application of the neural network**. To discuss the concept, let us first consider on the case of a neural network with a single hidden layer; the case of multiple hiddent layers can be discussed analogously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Consider the neural network\n",
    "$$\n",
    "    \\mathcal{N}_\\theta (x) = A^\\top \\sigma \\big( W x + b \\big),\n",
    "$$\n",
    "where $W \\in \\mathbb{R}^{m \\times d}$, $b \\in \\mathbb{R}^m$, and $A \\in \\mathbb{R}^m$,. We can split the weights and biases into $P$ parts $W_1, \\ldots, W_P$ and $b_1, \\ldots, b_P$ as follows\n",
    "$$\n",
    "    W\n",
    "    =\n",
    "    \\begin{bmatrix}\n",
    "        W_1 \\\\\n",
    "        \\vdots \\\\\n",
    "        W_P\n",
    "    \\end{bmatrix},\n",
    "    \\quad\n",
    "    b\n",
    "    =\n",
    "    \\begin{bmatrix}\n",
    "        b_1 \\\\\n",
    "        \\vdots \\\\\n",
    "        b_P\n",
    "    \\end{bmatrix},\n",
    "    \\quad \\text{and} \\quad\n",
    "    A\n",
    "    =\n",
    "    \\begin{bmatrix}\n",
    "        A_1 \\\\\n",
    "        \\vdots \\\\\n",
    "        A_P\n",
    "    \\end{bmatrix}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Then, we can write the application of the neural network as follows\n",
    "$$\n",
    "    \\mathcal{N}_\\theta (x)\n",
    "    =\n",
    "    \\begin{bmatrix}\n",
    "        A_1^\\top & \\cdots & A_P^\\top\n",
    "    \\end{bmatrix}\n",
    "    \\cdot\n",
    "    \\begin{bmatrix}\n",
    "        \\sigma \\big( W_1 x + b_1 \\big) \\\\\n",
    "        \\vdots \\\\\n",
    "        \\sigma \\big( W_P x + b_P \\big)\n",
    "    \\end{bmatrix}\n",
    "    =\n",
    "    \\sum_{p=1}^{P} \\underbrace{A_p^\\top \\sigma \\big( W_p x + b_p \\big)}_{=: \\mathcal{N}_{p,\\theta_p} (x)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Each part of the sum can be computed independently**, the application of a single layer can be written as the sum of individual models $\\mathcal{N}_{p,\\theta_p} (x)$. This can be implemented using the `pmap` function in Jax. We will skip this for the sake of brevity.\n",
    "\n",
    "In distributed-memory contexts, this can be implemented using different devices, and communication between the individual devices is required in two steps:\n",
    "1. We have to **communicate the input vector $x$ to all devices**.\n",
    "2. We have to **communicate and sum up the outputs of the individual devices**.\n",
    "\n",
    "In shared-memory contexts, no communication is required, but we have to synchronize the computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For the computation of the gradient of the loss function, we have to compute the gradient of the loss function $\\mathcal{L}(\\theta)$ in parallel, which is simple due to the linearity of the gradient:\n",
    "$$\n",
    "    \\nabla \\mathcal{L}(\\theta) \n",
    "    = \n",
    "    \\nabla \\frac{1}{n} \\sum_{i=1}^{n} \\left\\| \\mathcal{N}_\\theta (x_i) - y_i \\right\\|^2\n",
    "    = \n",
    "    \\nabla \\frac{1}{n} \\sum_{i=1}^{n} \\left\\| \\sum_{p=1}^{P} \\mathcal{N}_{p,\\theta_p} (x_i) - y_i \\right\\|^2\n",
    "    = \n",
    "    \\frac{1}{n} \\sum_{i=1}^{n} \\nabla \\left\\| \\sum_{p=1}^{P} \\mathcal{N}_{p,\\theta_p} (x_i) - y_i \\right\\|^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Looking at the parameters $\\theta_p$ and only one data point of the sum, we have\n",
    "$$\n",
    "    \\nabla_{\\theta_p} L(\\theta)\n",
    "    =\n",
    "    \\nabla_{\\theta_p} \\left\\| \\sum_{p=1}^{P} \\mathcal{N}_{p,\\theta_p} (x_i) - y_i \\right\\|^2\n",
    "    =\n",
    "    \\nabla_{\\theta_p} \\left( \\sum_{p=1}^{P} \\mathcal{N}_{p,\\theta_p} (x_i) - y_i \\right)^\\top \\left( \\sum_{p=1}^{P} \\mathcal{N}_{p,\\theta_p} (x_i) - y_i \\right)\n",
    "$$\n",
    "and\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "        \\nabla_{\\theta_p} L(\\theta) \n",
    "        & =\n",
    "        \\nabla_{\\theta_p}\n",
    "        \\sum_{p,q} \\left( \\mathcal{N}_{p,\\theta_p} (x_i) \\right)^\\top \\mathcal{N}_{q,\\theta_p} (x_i) - 2 \\sum_p \\left( \\mathcal{N}_{p,\\theta_p} (x_i) \\right)^\\top y_i + y_i^\\top y_i %\\\\        \n",
    "        %& =\n",
    "        %\\sum_{q} \\nabla_{\\theta_p} \\left( \\mathcal{N}_{p,\\theta_p} (x_i) \\right)^\\top \\mathcal{N}_{q,\\theta_p} (x_i) + \\sum_{q} \\left( \\mathcal{N}_{p,%\\theta_p} (x_i) \\right)^\\top \\nabla_{\\theta_p} \\mathcal{N}_{q,\\theta_p} (x_i) - 2 \\left( \\mathcal{N}_{p,\\theta_p} (x_i) \\right)^\\top y_i,\n",
    "    \\end{aligned}\n",
    "$$\n",
    "and we observe that **once $\\mathcal{N}_{p,\\theta_p} (x_i)$ and $\\nabla_{\\theta_p} \\mathcal{N}_{p,\\theta_p} (x_i)$ has been computed for each $p$ and communicated to all devices**, the computation of the gradient can be done in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This means that the **communication required can be considerable**, but computations can be done in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Before we move on to domain decomposition methods, we only briefly mention the **\"model and data parallelization\"** concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model and Data Parallelism\n",
    "\n",
    "From the previous sections, we could see that \n",
    "+ **data parallelism** is particularly useful if the **number of data samples is very large**,\n",
    "+ **model parallelism** is particularly useful if the **model is very large**.\n",
    "\n",
    "In the model and data parallelism approach, both the model and the data are split into different parts. Then, the **aforementioned concepts can be combined for more parallelism**. This can be useful if **both the model and the data are very large**, but a really efficient parallel implementation then might require more thought."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Domain Decomposition Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now discuss how **domain decomposition approaches** can be employed to **decompose the neural network architecture** itself. This can be useful both \n",
    "+ in the context of **function approximation** as well as\n",
    "+ in the context of **solving differential equations using physics-informed neural networks**.\n",
    "\n",
    "We will motivate the idea based on domain decomposition methods for partial differential equations and then discuss how this can be applied to neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Alternating Schwarz Method\n",
    "\n",
    "Historical remarks: The alternating Schwarz method is the earliest domain decomposition method (DDM), which has been invented by H. A. Schwarz and published in 1870:\n",
    "+ Schwarz used the algorithm to **establish the existence of harmonic functions** with prescribed boundary values on regions with **nonsmooth boundaries**.\n",
    "+ The regions are constructed recursively by forming unions of pairs of regions starting with \"simple\" regions for which existence can be established by more elementary means.\n",
    "+ At the **core of Schwarz's work** is a proof that solving in an **alternating way on the simple subregions** yields an **iterative scheme which converges at a geometric rate**.\n",
    "\n",
    "<img src=\"fig/doorknob.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In particular, the alternating Schwarz method can be used to solve the Poisson equation\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "        - \\Delta u & = f \\quad \\text{in} \\quad \\Omega, \\\\\n",
    "        u & = 0 \\quad \\text{on} \\quad \\partial \\Omega,\n",
    "    \\end{aligned}\n",
    "$$\n",
    "\n",
    "<img src=\"fig/doorknob.png\" width=\"400\">\n",
    "\n",
    "Here, the domain $\\Omega$ is decomposed into two overlapping subdomains $\\Omega_1$ and $\\Omega_2$. In the alternating Schwarz method, we **solve a series of local problems** on the subdomains $\\Omega_1$ and $\\Omega_2$ and **update the solution on the overlapping region** $\\Omega_1 \\cap \\Omega_2$ in each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In particular, in each iteration of the method, we solve the following local problems:\n",
    "$$\n",
    "    \\begin{array}{ccc}\n",
    "        \\begin{array}{rcl}\n",
    "            - {u_1^{(k)}}'' & = & 1 \\quad \\text{in} \\quad \\Omega_1, \\\\\n",
    "            u_1^{(k)} & = & 0 \\quad \\text{on} \\quad \\partial \\Omega \\cap \\partial \\Omega_1, \\\\\n",
    "            u_1^{(k)} & = & u_2^{(k-1)} \\quad \\text{on} \\quad \\partial \\Omega_1, \\\\\n",
    "        \\end{array}\n",
    "        &\n",
    "        \\qquad \\text{and} \\qquad \n",
    "        &\n",
    "        \\begin{array}{rcl}\n",
    "            - {u_2^{(k)}}'' & = & 1 \\quad \\text{in} \\quad \\Omega_2, \\\\\n",
    "            u_2^{(k)} & = & 0 \\quad \\text{on} \\quad \\partial \\Omega \\cap \\partial \\Omega_2, \\\\\n",
    "            u_2^{(k)} & = & u_1^{(k)} \\quad \\text{on} \\quad \\partial \\Omega_2, \\\\\n",
    "        \\end{array}\n",
    "    \\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that \n",
    "+ the solution of $u_1$ in the $k$-th iteration depends on $u_2$ in the $(k-1)$-th iteration and\n",
    "+ the solution of $u_2$ in the $k$-th iteration depends on $u_1$ in the $k$-th iteration.\n",
    "\n",
    "This is the reason why the method is called **alternating Schwarz method**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### One Dimensional Example of the Alternating Schwarz Method in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to better understand the concept, we will consider the following simple **one-dimensional example**. Let $\\Omega = (0,1)$ and consider the Poisson equation on that domain:\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "        - u'' & = 1 \\quad \\text{in} \\quad \\Omega, \\\\\n",
    "        u & = 0 \\quad \\text{on} \\quad \\partial \\Omega.\n",
    "    \\end{aligned}\n",
    "$$\n",
    "\n",
    "To apply the alternating Schwarz method, we decompose $\\Omega$ ito $\\Omega_1 = (0,1/2+\\delta)$ and $\\Omega_2 = (1/2-\\delta,1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For the one-dimensional example under consideration, the **solutions of the local problems can be computed analytically**. In particular, for subdomain $\\Omega_1$, the solution of\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "        - {u_1}'' & = 1 \\quad \\text{in} \\quad \\Omega_1, \\\\\n",
    "        u_1 (0) & = 0, \\\\\n",
    "        u_1 (1/2+\\delta) & = a, \n",
    "    \\end{aligned}\n",
    "$$\n",
    "reads can be written as\n",
    "$$\n",
    "    u_1 (x) = u_{pde} (x) + u_{bc} (x),\n",
    "$$\n",
    "where $u_{bc} (x)$ is **linear and satisfies the boundary conditions** and $u_{pde} (x)$ is the solution of the Poisson equation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "In particular, we have\n",
    "$$\n",
    "    u_{pde} (x) = \\frac{1}{2} x (1-x) \n",
    "    \\quad \\Rightarrow \\quad\n",
    "    u_{pde} (0) = 0 \n",
    "    \\quad \\text{and} \\quad\n",
    "    - u_{pde}'' (x) = 1.\n",
    "$$\n",
    "Then, \n",
    "$$\n",
    "    \\begin{aligned}\n",
    "        && u_{bc} (x) & = \\frac{a - u_{pde}(1/2+\\delta)}{1/2+\\delta} x , \\\\\n",
    "        \\Rightarrow && {u_{bc}}'' & = 0, \\quad\n",
    "        u_{bc} (0) = 0, \n",
    "        \\quad \\text{and} \\quad\n",
    "        u_{bc} (1/2+\\delta) & = a - u_{pde}(1/2+\\delta), \n",
    "    \\end{aligned}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As a result, we obtain for $u_1 (x) = u_{pde} (x) + u_{bc} (x)$ that \n",
    "$$\n",
    "    -{u_1}'' = 1,\n",
    "    \\quad\n",
    "    u_1 (0) = 0,\n",
    "    \\quad \\text{and} \\quad\n",
    "    u_1 (1/2+\\delta) = a.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Similarly, we can obtain the **solution of the second local problem** \n",
    "$$\n",
    "    \\begin{aligned}\n",
    "        - {u_2}'' & = 1 \\quad \\text{in} \\quad \\Omega_1, \\\\\n",
    "        u_2 (0) & = 0, \\\\\n",
    "        u_2 (1/2+\\delta) & = b, \n",
    "    \\end{aligned}\n",
    "$$\n",
    "on subdomain $\\Omega_2$:\n",
    "$$\n",
    "    u_2 (x) = \\frac{1}{2} x (1-x) + \\frac{b - u_{pde}(1/2+\\delta)}{1/2+\\delta} (1 - x).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now, we will consider a example in Jax. Therefore, we start with the **initial guess**\n",
    "$$\n",
    "    u^{(0)} = 0\n",
    "$$\n",
    "and\n",
    "$$\n",
    "    u_1^{(0)} = u^{(0)} = 0\n",
    "    \\quad \\text{and} \\quad\n",
    "    u_2^{(0)} = u^{(0)} = 0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def u_pde(x):\n",
    "    return 0.5*x*(1-x)\n",
    "\n",
    "def u_1(x, delta, a):\n",
    "    return u_pde(x) + (a-u_pde(0.5+delta))/(0.5+delta)*x\n",
    "\n",
    "def u_2(x, delta, b):\n",
    "    return u_pde(x) + (b-u_pde(0.5+delta))/(0.5+delta)*(1-x)\n",
    "\n",
    "def alternating_schwarz(delta, n):\n",
    "    a = 0\n",
    "    b = u_1(0.5-delta, delta, a)\n",
    "    for i in range(n-1):\n",
    "        a = u_2(0.5+delta, delta, b)\n",
    "        b = u_1(0.5-delta, delta, a)\n",
    "\n",
    "    X_1 = np.linspace(0, 0.5+delta, 100)\n",
    "    U_1 = u_1(X_1, delta, a)\n",
    "    X_2 = np.linspace(0.5-delta, 1, 100)\n",
    "    U_2 = u_2(X_2, delta, b)\n",
    "    \n",
    "    return X_1, U_1, X_2, U_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now, we compute and plot the iterates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "delta = 0.1\n",
    "\n",
    "x1 = np.linspace(0, 0.5+delta, 100)\n",
    "x2 = np.linspace(0.5-delta, 1, 100)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.set_xlim(0., 1)\n",
    "ax.set_ylim(0., 0.14)\n",
    "line1a, = ax.plot(x1, 0.*x1, 'r--')\n",
    "line1b, = ax.plot(x1, 0.*x1, 'r')\n",
    "line2a, = ax.plot(x2, 0.*x2, 'b--')\n",
    "line2b, = ax.plot(x2, 0.*x2, 'b')\n",
    "\n",
    "def update(delta = 0.1, n = 0):\n",
    "    if n == 0:\n",
    "        x1 = np.linspace(0, 0.5+delta, 100)\n",
    "        x2 = np.linspace(0.5-delta, 1, 100)\n",
    "        line1a.set_xdata(x1)\n",
    "        line1a.set_ydata(0.*x1)\n",
    "        line1b.set_xdata(x1)\n",
    "        line1b.set_ydata(0.*x1)\n",
    "        line2a.set_xdata(x2)\n",
    "        line2a.set_ydata(0.*x2)\n",
    "        line2b.set_xdata(x2)\n",
    "        line2b.set_ydata(0.*x2)\n",
    "    else:\n",
    "        X_1a, U_1a, X_2a, U_2a = alternating_schwarz(delta, n-1)\n",
    "        X_1b, U_1b, X_2b, U_2b = alternating_schwarz(delta, n)\n",
    "        if n == 1:\n",
    "            U_1a = 0.*U_1a\n",
    "            U_2a = 0.*U_2a\n",
    "        line1a.set_xdata(X_1a)\n",
    "        line1a.set_ydata(U_1a)\n",
    "        line1b.set_xdata(X_1b)\n",
    "        line1b.set_ydata(U_1b)\n",
    "        line2a.set_xdata(X_2a)\n",
    "        line2a.set_ydata(U_2a)\n",
    "        line2b.set_xdata(X_2b)\n",
    "        line2b.set_ydata(U_2b)\n",
    "\n",
    "    fig.canvas.draw_idle()\n",
    "\n",
    "interact(update, delta=(0.,0.5,0.1), n=(0,20,1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We observe that the **solution converges to the exact solution of the Poisson equation**. Moreover, a **larger overlap** of the subdomains leads to **faster convergence**.\n",
    "\n",
    "Moreover, the algoerithm is sequential with respect to the solution of the local problems on the subdomains. This is because the solution of the first subdomain problem requires the solution of the other subdomain problem from the previous iteration:\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "        - {u_1^{(k)}}'' & = 1 \\quad \\text{in} \\quad \\Omega_1, \\\\\n",
    "        u_1^{(k)} & = 0 \\quad \\text{on} \\quad \\partial \\Omega \\cap \\partial \\Omega_1, \\\\\n",
    "        u_1^{(k)} & = u_2^{(k-1)} \\quad \\text{on} \\quad \\partial \\Omega_1, \\\\\n",
    "    \\end{aligned}\n",
    "$$\n",
    "and vice versa:\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "        - {u_2^{(k)}}'' & = 1 \\quad \\text{in} \\quad \\Omega_2, \\\\\n",
    "        u_2^{(k)} & = 0 \\quad \\text{on} \\quad \\partial \\Omega \\cap \\partial \\Omega_2, \\\\\n",
    "        u_2^{(k)} & = u_1^{(k)} \\quad \\text{on} \\quad \\partial \\Omega_2, \\\\\n",
    "    \\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Can it be parallelized? How do we have to modify the algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Parallel Schwarz Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we extend this idea to the parallel Schwarz algorithm. In the parallel Schwarz algorithm, we solve the local problems on the subdomains in parallel, which only requires a minor modification of the algorithm. In particular, in each iteration of the method, we solve the following local problems:\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "        - {u_1^{(k)}}'' & = 1 \\quad \\text{in} \\quad \\Omega_1, \\\\\n",
    "        u_1^{(k)} & = 0 \\quad \\text{on} \\quad \\partial \\Omega \\cap \\partial \\Omega_1, \\\\\n",
    "        u_1^{(k)} & = u_2^{(k-1)} \\quad \\text{on} \\quad \\partial \\Omega_1, \\\\\n",
    "    \\end{aligned}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "        - {u_2^{(k)}}'' & = 1 \\quad \\text{in} \\quad \\Omega_2, \\\\\n",
    "        u_2^{(k)} & = 0 \\quad \\text{on} \\quad \\partial \\Omega \\cap \\partial \\Omega_2, \\\\\n",
    "        u_2^{(k)} & = u_1^{(k-1)} \\quad \\text{on} \\quad \\partial \\Omega_2, \\\\\n",
    "    \\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two problems are now completely parallelized because the solution of the first subdomain problem does not depend on the solution of the other subdomain problem from the previous iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### One Dimensional Example of the Parallel Schwarz Method in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the previously derived analytical solutions, we can now directly **implement the parallel Schwarz algorithm**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def u_pde(x):\n",
    "    return 0.5*x*(1-x)\n",
    "\n",
    "def u_1(x, delta, a):\n",
    "    return u_pde(x) + (a-u_pde(0.5+delta))/(0.5+delta)*x\n",
    "\n",
    "def u_2(x, delta, b):\n",
    "    return u_pde(x) + (b-u_pde(0.5+delta))/(0.5+delta)*(1-x)\n",
    "\n",
    "def parallel_schwarz(delta, n):\n",
    "    a_prev = 0\n",
    "    b_prev = 0\n",
    "    for i in range(n-1):\n",
    "        b = u_1(0.5-delta, delta, a_prev)\n",
    "        a = u_2(0.5+delta, delta, b_prev)\n",
    "        a_prev = a\n",
    "        b_prev = b\n",
    "\n",
    "    X_1 = np.linspace(0, 0.5+delta, 100)\n",
    "    U_1 = u_1(X_1, delta, a_prev)\n",
    "    X_2 = np.linspace(0.5-delta, 1, 100)\n",
    "    U_2 = u_2(X_2, delta, b_prev)\n",
    "    \n",
    "    return X_1, U_1, X_2, U_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now, we compute and plot the iterates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "delta = 0.1\n",
    "\n",
    "x1 = np.linspace(0, 0.5+delta, 100)\n",
    "x2 = np.linspace(0.5-delta, 1, 100)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.set_xlim(0., 1)\n",
    "ax.set_ylim(0., 0.14)\n",
    "line1a, = ax.plot(x1, 0.*x1, 'r--')\n",
    "line1b, = ax.plot(x1, 0.*x1, 'r')\n",
    "line2a, = ax.plot(x2, 0.*x2, 'b--')\n",
    "line2b, = ax.plot(x2, 0.*x2, 'b')\n",
    "\n",
    "def update(delta = 0.1, n = 0):\n",
    "    if n == 0:\n",
    "        x1 = np.linspace(0, 0.5+delta, 100)\n",
    "        x2 = np.linspace(0.5-delta, 1, 100)\n",
    "        line1a.set_xdata(x1)\n",
    "        line1a.set_ydata(0.*x1)\n",
    "        line1b.set_xdata(x1)\n",
    "        line1b.set_ydata(0.*x1)\n",
    "        line2a.set_xdata(x2)\n",
    "        line2a.set_ydata(0.*x2)\n",
    "        line2b.set_xdata(x2)\n",
    "        line2b.set_ydata(0.*x2)\n",
    "    else:\n",
    "        X_1a, U_1a, X_2a, U_2a = parallel_schwarz(delta, n-1)\n",
    "        X_1b, U_1b, X_2b, U_2b = parallel_schwarz(delta, n)\n",
    "        if n == 1:\n",
    "            U_1a = 0.*U_1a\n",
    "            U_2a = 0.*U_2a     \n",
    "        line1a.set_xdata(X_1a)\n",
    "        line1a.set_ydata(U_1a)\n",
    "        line1b.set_xdata(X_1b)\n",
    "        line1b.set_ydata(U_1b)\n",
    "        line2a.set_xdata(X_2a)\n",
    "        line2a.set_ydata(U_2a)\n",
    "        line2b.set_xdata(X_2b)\n",
    "        line2b.set_ydata(U_2b)\n",
    "\n",
    "    fig.canvas.draw_idle()\n",
    "\n",
    "interact(update, delta=(0.,0.5,0.1), n=(0,20,1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Comparison of the Two Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we **compare the two methods** next to each other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "delta = 0.1\n",
    "\n",
    "x1 = np.linspace(0, 0.5+delta, 100)\n",
    "x2 = np.linspace(0.5-delta, 1, 100)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "ax[0].set_xlim(0., 1)\n",
    "ax[0].set_ylim(0., 0.14)\n",
    "as_line1a, = ax[0].plot(x1, 0.*x1, 'r--')\n",
    "as_line1b, = ax[0].plot(x1, u_1(x1, delta, 0), 'r')\n",
    "as_line2a, = ax[0].plot(x2, 0.*x2, 'b--')\n",
    "as_line2b, = ax[0].plot(x2, 0.*x2, 'b')\n",
    "\n",
    "ax[1].set_xlim(0., 1)\n",
    "ax[1].set_ylim(0., 0.14)\n",
    "ps_line1a, = ax[1].plot(x1, 0.*x1, 'r--')\n",
    "ps_line1b, = ax[1].plot(x1, u_1(x1, delta, 0), 'r')\n",
    "ps_line2a, = ax[1].plot(x2, 0.*x2, 'b--')\n",
    "ps_line2b, = ax[1].plot(x2, u_2(x2, delta, 0), 'b')\n",
    "\n",
    "def update(delta = 0.1, n = 0):\n",
    "    if n == 0:\n",
    "        x1 = np.linspace(0, 0.5 + delta, 100)\n",
    "        x2 = np.linspace(0.5-delta, 1, 100)\n",
    "\n",
    "        as_line1a.set_xdata(x1)\n",
    "        as_line1a.set_ydata(0.*x1)\n",
    "        as_line1b.set_xdata(x1)\n",
    "        as_line1b.set_ydata(0.*x1)\n",
    "        as_line2a.set_xdata(x2)\n",
    "        as_line2a.set_ydata(0.*x2)\n",
    "        as_line2b.set_xdata(x2)\n",
    "        as_line2b.set_ydata(0.*x2)\n",
    "\n",
    "        ps_line1a.set_xdata(x1)\n",
    "        ps_line1a.set_ydata(0.*x1)\n",
    "        ps_line1b.set_xdata(x1)\n",
    "        ps_line1b.set_ydata(0.*x1)\n",
    "        ps_line2a.set_xdata(x2)\n",
    "        ps_line2a.set_ydata(0.*x2)\n",
    "        ps_line2b.set_xdata(x2)\n",
    "        ps_line2b.set_ydata(0.*x2)\n",
    "    else:\n",
    "        as_X_1a, as_U_1a, as_X_2a, as_U_2a = alternating_schwarz(delta, n-1)\n",
    "        as_X_1b, as_U_1b, as_X_2b, as_U_2b = alternating_schwarz(delta, n)\n",
    "        if n == 1:\n",
    "            as_U_1a = 0.*as_U_1a\n",
    "            as_U_2a = 0.*as_U_2a\n",
    "        as_line1a.set_xdata(as_X_1a)\n",
    "        as_line1a.set_ydata(as_U_1a)\n",
    "        as_line1b.set_xdata(as_X_1b)\n",
    "        as_line1b.set_ydata(as_U_1b)\n",
    "        as_line2a.set_xdata(as_X_2a)\n",
    "        as_line2a.set_ydata(as_U_2a)\n",
    "        as_line2b.set_xdata(as_X_2b)\n",
    "        as_line2b.set_ydata(as_U_2b)\n",
    "\n",
    "        ps_X_1a, ps_U_1a, ps_X_2a, ps_U_2a = parallel_schwarz(delta, n-1)\n",
    "        ps_X_1b, ps_U_1b, ps_X_2b, ps_U_2b = parallel_schwarz(delta, n)\n",
    "        if n == 1:\n",
    "            ps_U_1a = 0.*ps_U_1a\n",
    "            ps_U_2a = 0.*ps_U_2a\n",
    "        ps_line1a.set_xdata(ps_X_1a)\n",
    "        ps_line1a.set_ydata(ps_U_1a)\n",
    "        ps_line1b.set_xdata(ps_X_1b)\n",
    "        ps_line1b.set_ydata(ps_U_1b)\n",
    "        ps_line2a.set_xdata(ps_X_2a)\n",
    "        ps_line2a.set_ydata(ps_U_2a)\n",
    "        ps_line2b.set_xdata(ps_X_2b)\n",
    "        ps_line2b.set_ydata(ps_U_2b)\n",
    "\n",
    "    fig.canvas.draw_idle()\n",
    "\n",
    "interact(update, delta=(0.,0.5,0.1), n=(0,20,1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We clearly observe that \n",
    "+ the alternating Schwarz methods **converges much faster in terms of the number of iterations**, \n",
    "+ whereas in the parallel Schwarz method, both subdomain solves can be carried out in parallel. \n",
    "\n",
    "In practice, we observe that the **alternating Schwarz method converges exactly twice as fast as the parallel Schwarz method**. Therefore, if each of the subdomain problems is solved on a separate device, they will converge in the same time, if we assume that the communication needed for both methods is comparable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Final Comments on Domain Decomposition Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parallel Schwarz iteration becomes particularly interesting in the case of **more than two subdomains**, allowing for the solution of very large problems in parallel:\n",
    "+ If the problem size is increased at the same rate as the number of subdomains, the **cost for each local solve remains the same**.\n",
    "+ One can observe: in a parallel computation, the computing time of each iteration of the the alternating Schwarz method **increases linearly with the number of subdomains**, whereas the time of the parallel Schwarz method **remains constant** (neglecting communication).\n",
    "+ Unfortunately, the **convergence rate of the parallel Schwarz iteration** will **decrease when the number of subdomains is increased**, because information can only travel to the neighboring subdomains within one iteration.\n",
    "\n",
    "\n",
    "Moreover, there are also other domain decomposition methods, in particular a class of methods that are based on a **nonoverlapping decomposition of the computational domain**. \n",
    "\n",
    "_Here, we will only use the idea of classical domain decomposition solvers as a motivation to design decomposed neural network architectures; they will be covered in more detail in the **Computational Fluid Dynamics** course._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Domain Decomposition for Function Approximation Using Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will discuss about how to decompose neural networks to **approximate (nonlinear) functions**. In the final part of this lecture, we will then move on to discuss how to **decompose neural networks to solve partial differential equations using physics-informed neural networks**, which will require additional care."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training Performance of Neural Networks For Approximating Functions with Varying Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already observed that function approximation using neural networks is more challenging if the function to be approximated is highly oscillating. We will perform a more systematic study using the `FeedForwardNN` class from above.\n",
    "\n",
    "In particular, we will investigate what happens if we increase the frequency of the function to be approximated the same time as the width of a neural network with a single hidden layer. In this case, the function approximation should always be possible with the same accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let us investigate this using a Python example. We first define some required functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a neural network\n",
    "key = random.PRNGKey(0)\n",
    "layer_sizes = [1, 10, 1] \n",
    "nn1 = FeedForwardNN(layer_sizes, key)\n",
    "\n",
    "# Define the loss function (Mean Squared Error)\n",
    "squared_error = jit(lambda params, x, y: (nn1.forward(params, x) - y) ** 2)\n",
    "squared_errors_batch = vmap(squared_error, in_axes=(None, 0, 0))\n",
    "@jit\n",
    "def loss_fn(params, x, y):\n",
    "    return jnp.mean(squared_errors_batch(params, x, y))\n",
    "\n",
    "# Define training step with Adam optimizer\n",
    "def train_step(params, opt_state, x, y):    \n",
    "    loss, grads = jax.value_and_grad(loss_fn)(params, x, y)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return new_params, opt_state, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " We start with the function $\\sin(\\pi x)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sin(pi*x) function\n",
    "x = jnp.linspace(-1.0, 1.0, 1000)\n",
    "y1 = jnp.sin(jnp.pi*x)\n",
    "\n",
    "# Initialize the Adam optimizer with the learning rate schedule\n",
    "learning_rate_schedule = optax.piecewise_constant_schedule(\n",
    "    init_value=0.1,\n",
    "    boundaries_and_scales={500: 0.1}\n",
    ")\n",
    "optimizer = optax.adam(learning_rate=learning_rate_schedule)\n",
    "opt_state = optimizer.init(nn1.params)\n",
    "losses1 = []\n",
    "\n",
    "# Training loop\n",
    "max_iterations1 = 20000\n",
    "\n",
    "# Training loop with tqdm progress bar\n",
    "pbar = trange(max_iterations1, desc=\"Training\", leave=True)\n",
    "for epoch in pbar:\n",
    "    nn1.params, opt_state, current_loss = train_step(nn1.params, opt_state, x, y1)\n",
    "    losses1.append(current_loss)\n",
    "    if epoch % 100 == 0:\n",
    "        pbar.set_postfix(loss=current_loss)\n",
    "    if current_loss < 0.001:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Next, we consider the function $\\sin(4 \\pi x)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# sin(4*pi*x) function\n",
    "y2 = jnp.sin(4*jnp.pi*x)\n",
    "\n",
    "# Create a neural network\n",
    "key = random.PRNGKey(0)\n",
    "layer_sizes = [1, 40, 1] \n",
    "nn2 = FeedForwardNN(layer_sizes, key)\n",
    "\n",
    "# Define the loss function (Mean Squared Error)\n",
    "squared_error = jit(lambda params, x, y: (nn2.forward(params, x) - y) ** 2)\n",
    "squared_errors_batch = vmap(squared_error, in_axes=(None, 0, 0))\n",
    "@jit\n",
    "def loss_fn(params, x, y):\n",
    "    return jnp.mean(squared_errors_batch(params, x, y))\n",
    "\n",
    "# Define training step with Adam optimizer\n",
    "def train_step(params, opt_state, x, y):    \n",
    "    loss, grads = jax.value_and_grad(loss_fn)(params, x, y)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return new_params, opt_state, loss\n",
    "\n",
    "# Initialize the Adam optimizer with the learning rate schedule\n",
    "learning_rate_schedule = optax.piecewise_constant_schedule(\n",
    "    init_value=0.1,\n",
    "    boundaries_and_scales={2000: 0.1, 6000: 0.1}\n",
    ")\n",
    "optimizer = optax.adam(learning_rate=learning_rate_schedule)\n",
    "opt_state = optimizer.init(nn2.params)\n",
    "losses2 = []\n",
    "\n",
    "# Training loop\n",
    "max_iterations2 = 20000\n",
    "\n",
    "# Training loop with tqdm progress bar\n",
    "pbar = trange(max_iterations2, desc=\"Training\", leave=True)\n",
    "for epoch in pbar:\n",
    "    nn2.params, opt_state, current_loss = train_step(nn2.params, opt_state, x, y2)\n",
    "    losses2.append(current_loss)\n",
    "    if epoch % 100 == 0:\n",
    "        pbar.set_postfix(loss=current_loss)\n",
    "    if current_loss < 0.001:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finally, we consider the function $\\sin(16 \\pi x)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# sin(16*pi*x) function\n",
    "y3 = jnp.sin(16*jnp.pi*x)\n",
    "\n",
    "# Create a neural network\n",
    "key = random.PRNGKey(0)\n",
    "layer_sizes = [1, 160, 1] \n",
    "nn3 = FeedForwardNN(layer_sizes, key)\n",
    "\n",
    "# Define the loss function (Mean Squared Error)\n",
    "squared_error = jit(lambda params, x, y: (nn3.forward(params, x) - y) ** 2)\n",
    "squared_errors_batch = vmap(squared_error, in_axes=(None, 0, 0))\n",
    "@jit\n",
    "def loss_fn(params, x, y):\n",
    "    return jnp.mean(squared_errors_batch(params, x, y))\n",
    "\n",
    "# Define training step with Adam optimizer\n",
    "def train_step(params, opt_state, x, y):    \n",
    "    loss, grads = jax.value_and_grad(loss_fn)(params, x, y)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return new_params, opt_state, loss\n",
    "\n",
    "# Initialize the Adam optimizer with the learning rate schedule\n",
    "learning_rate_schedule = optax.piecewise_constant_schedule(\n",
    "    init_value=0.01,\n",
    ")\n",
    "optimizer = optax.adam(learning_rate=learning_rate_schedule)\n",
    "opt_state = optimizer.init(nn3.params)\n",
    "losses3 = []\n",
    "\n",
    "# Training loop\n",
    "max_iterations3 = 20000\n",
    "\n",
    "# Training loop with tqdm progress bar\n",
    "pbar = trange(max_iterations3, desc=\"Training\", leave=True)\n",
    "for epoch in pbar:\n",
    "    nn3.params, opt_state, current_loss = train_step(nn3.params, opt_state, x, y3)\n",
    "    losses3.append(current_loss)\n",
    "    if epoch % 100 == 0:\n",
    "        pbar.set_postfix(loss=current_loss)\n",
    "    if current_loss < 0.001:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now, we plot all the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, axs = plt.subplots(3, 2, figsize=(20, 10))\n",
    "\n",
    "# Plot the loss over the number of iterations for nn1\n",
    "axs[0, 0].set_yscale('log')\n",
    "axs[0, 0].plot(range(len(losses1)), losses1, label='Loss')\n",
    "axs[0, 0].set_xlabel('Iteration')\n",
    "axs[0, 0].set_ylabel('Loss')\n",
    "axs[0, 0].legend()\n",
    "\n",
    "# Plot the results for nn1\n",
    "axs[0, 1].plot(x, y1, label='Sine function')\n",
    "axs[0, 1].plot(x, nn1.predict(x).reshape(y1.shape), label='Neural network')\n",
    "axs[0, 1].legend()\n",
    "\n",
    "# Plot the loss over the number of iterations for nn2\n",
    "axs[1, 0].set_yscale('log')\n",
    "axs[1, 0].plot(range(len(losses2)), losses2, label='Loss')\n",
    "axs[1, 0].set_xlabel('Iteration')\n",
    "axs[1, 0].set_ylabel('Loss')\n",
    "axs[1, 0].legend()\n",
    "\n",
    "# Plot the results for nn2\n",
    "axs[1, 1].plot(x, y2, label='Sine function')\n",
    "axs[1, 1].plot(x, nn2.predict(x).reshape(y2.shape), label='Neural network')\n",
    "axs[1, 1].legend()\n",
    "\n",
    "# Plot the loss over the number of iterations for nn3\n",
    "axs[2, 0].set_yscale('log')\n",
    "axs[2, 0].plot(range(len(losses3)), losses3, label='Loss')\n",
    "axs[2, 0].set_xlabel('Iteration')\n",
    "axs[2, 0].set_ylabel('Loss')\n",
    "axs[2, 0].legend()\n",
    "\n",
    "# Plot the results for nn3\n",
    "axs[2, 1].plot(x, y3, label='Sine function')\n",
    "axs[2, 1].plot(x, nn3.predict(x).reshape(y3.shape), label='Neural network')\n",
    "axs[2, 1].legend()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So, the it is **not sufficient to scale up the number of neurons with the frequency of the function to be approximated**. However, there is a simple way to improve the approximation performance of the neural network, based on the idea of domain decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A Simple Domain Decomposition Approach for Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of domain decomposition is to **split the problem into smaller subproblems**, based on a **decomposition of the spatial domain**. Here, we employ a domain decomposition approach to **decompose the input domain**, that is the one-dimensional interval $[-1,1]$, into $P$ subintervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let us assume that a single-layer neural network \n",
    "$$\n",
    "    \\mathcal{N}_\\theta (x) = A^\\top \\sigma \\big( W x + b \\big),\n",
    "$$\n",
    "where $W \\in \\mathbb{R}^{m}$, $b \\in \\mathbb{R}^m$, and $A \\in \\mathbb{R}^m$, can approximate the function $\\sin (\\pi x)$ well on the interval $[-1,1]$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Then, we would assume that a neural network\n",
    "$$\n",
    "    \\mathcal{N}_{\\hat \\theta} (x) = \\hat A^\\top \\sigma \\big(\\hat W x + \\hat b \\big),\n",
    "$$\n",
    "with $\\hat W \\in \\mathbb{R}^{km}$, $\\hat b \\in \\mathbb{R}^{km}$, and $\\hat A \\in \\mathbb{R}^{km}$, can approximate the function $\\sin (k \\pi x)$ well. But as we have seen, the situation is not as simple as that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Therefore, let us first **decompose the interval** $[-1,1]$ into $k$ subintervals $I_1, \\ldots, I_k$, with \n",
    "$$\n",
    "    I_j = \\big[ \\underbrace{\\frac{2(j-1)}{k}-1}_{=: x_j^s}, \\underbrace{\\frac{2j}{k}-1}_{=: x_j^e} \\big)\n",
    "$$\n",
    "In particular, let\n",
    "$$\n",
    "    \\omega_j \n",
    "    =\n",
    "    \\begin{cases}\n",
    "        1 & \\text{if} \\quad x \\in I_j, \\\\\n",
    "        0 & \\text{otherwise}.\n",
    "    \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that these functions are a **partition of unity**, i.e., \n",
    "$$\n",
    "    \\sum_{j=1}^{k} \\omega_j (x) = 1\n",
    "$$ \n",
    "for all $x \\in [0,1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Then, we define a **new neural network architecture** as follows\n",
    "$$\n",
    "    \\mathcal{N}_{\\theta} (x) = \\sum_{i=j}^{k} \\omega_j (x) \\mathcal{N}_{j,\\theta_j} (x),\n",
    "$$\n",
    "where $\\mathcal{N}_{j,\\theta_j} (x)$ is a neural network with an architecture that can approximate a single $\\sin (x)$ well, that is,\n",
    "$$\n",
    "    \\mathcal{N}_{i,\\theta_i} (x) = A_j^\\top \\sigma \\big( W_j x + b_j \\big),\n",
    "$$\n",
    "where $W_j \\in \\mathbb{R}^{m}$, $b_j \\in \\mathbb{R}^m$, and $A_j \\in \\mathbb{R}^m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This means that the support of the function\n",
    "$$\n",
    "    \\mathcal{F}_{j,\\theta_j} (x) = \\omega_j (x) \\mathcal{N}_{j,\\theta_j} (x)\n",
    "$$\n",
    "is only inside the interval $I_j$. Outside the interval, the function will be zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hence, if we evaluate the loss function \n",
    "$$\n",
    "    \\mathcal{L}(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} \\left\\| \\mathcal{N}_\\theta (x_i) - y_i \\right\\|^2\n",
    "$$\n",
    "with this network architecture, this can be rewritten as\n",
    "$$\n",
    "    \\mathcal{L}(\\theta) \n",
    "    =\n",
    "    \\frac{1}{n} \\sum_j \\left\\| \\sum_{i=1}^n \\mathcal{F}_{j,\\theta_j} (x_i) - y_i \\right\\|^2\n",
    "    =\n",
    "    \\frac{1}{n} \\sum_j \\left\\| \\sum_{x_i \\in I_j} \\mathcal{F}_{j,\\theta_j} (x_i) - y_i \\right\\|^2.\n",
    "$$\n",
    "In the last step, we have used that the support of the function $\\mathcal{F}_{j,\\theta_j} (x)$ is inside the interval $I_j$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This means that, for an **efficient implementation**, we do not even have to evaluate $\\mathcal{F}_{j,\\theta_j} (x_i)$ outside the interval $I_j$. For simplicity, we will not implement this here, but it is a simple modification of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Moreover, the network corresponding to a subinterval only has to consider **data points inside $I_j$**. This means that, for the network to learn the function on this interval well, we should normalize the input data to $\\mathcal{F}_{i,\\theta_i}$, for instance, to be $[-1,1]$; this will also **depend on the weight initialization**. As a consequence, we introduce a **normalization step**\n",
    "$$\n",
    "    {\\rm norm}(x) = \\frac{x - \\frac{1}{2}\\left(x_j^s + x_j^e\\right)}{\\frac{1}{2}\\left(x_j^e - x_j^s\\right)}\n",
    "$$\n",
    "and consider the final neural network architecture \n",
    "$$\n",
    "    \\mathcal{N}_{\\theta} (x) = \\sum_{i=j}^{k} \\omega_j (x) \\mathcal{N}_{j,\\theta_j} ({\\rm norm}(x)).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, **every individual network will only see a a single interval**, for instance, a single period of the sine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Python Example of Domain Decomposition for Function Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now implement exactly the architecture proposed in the previous section. We will consider the function $\\sin (16 \\pi x)$ and decompose the interval $[-1,1]$ into $k$ subintervals. We will then train the network to approximate the function using the domain decomposition approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define the neural network architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data\n",
    "k = 16\n",
    "x = jnp.linspace(-1.0, 1.0, 1000)\n",
    "y = jnp.sin(k * jnp.pi * x)\n",
    "\n",
    "# Define the domain decomposition neural network\n",
    "class DomainDecomposedNN:\n",
    "    def __init__(self, num_subdomains, layer_sizes, key, activation_fn=jax.nn.tanh):\n",
    "        self.num_subdomains = num_subdomains\n",
    "        self.subnets = [FeedForwardNN(layer_sizes, random.split(key, num_subdomains)[i], activation_fn) for i in range(num_subdomains)]\n",
    "        self.params = [subnet.params for subnet in self.subnets]\n",
    "\n",
    "    def forward(self, params, x):\n",
    "        subdomain_size = 2.0 / self.num_subdomains\n",
    "        outputs = []\n",
    "        for i in range(self.num_subdomains):\n",
    "            mask = jnp.where((x >= -1.0 + i * subdomain_size) & (x < -1.0 + (i + 1) * subdomain_size), 1.0, 0.0)\n",
    "            norm_x = 2 * (x - (-1.0 + (i + 0.5) * subdomain_size)) / subdomain_size\n",
    "            outputs.append(mask * self.subnets[i].forward(params[i], norm_x))\n",
    "        return jnp.sum(jnp.array(outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Then, we define the loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function (Mean Squared Error)\n",
    "squared_error = jit(lambda params, x, y: (nn.forward(params, x) - y) ** 2)\n",
    "squared_errors_batch = vmap(squared_error, in_axes=(None, 0, 0))\n",
    "@jit\n",
    "def loss_fn(params, x, y):    \n",
    "    return jnp.mean(squared_errors_batch(params, x, y))\n",
    "\n",
    "# Define training step with Adam optimizer\n",
    "def train_step(params, opt_state, x, y):\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(params, x, y)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return new_params, opt_state, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now, we train the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the domain decomposed neural network\n",
    "key = random.PRNGKey(0)\n",
    "layer_sizes = [1, 10, 1]\n",
    "nn = DomainDecomposedNN(k, layer_sizes, key)\n",
    "\n",
    "# Create a learning rate schedule\n",
    "learning_rate_schedule = optax.piecewise_constant_schedule(\n",
    "    init_value=0.1,\n",
    "    boundaries_and_scales={300: 0.5}\n",
    ")\n",
    "\n",
    "# Initialize the Adam optimizer with the learning rate schedule\n",
    "optimizer = optax.adam(learning_rate=learning_rate_schedule)\n",
    "opt_state = optimizer.init(nn.params)\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "max_iterations = 1000\n",
    "\n",
    "# Training loop with tqdm progress bar\n",
    "pbar = trange(max_iterations, desc=\"Training\", leave=True)\n",
    "for epoch in pbar:\n",
    "    nn.params, opt_state, current_loss = train_step(nn.params, opt_state, x, y)\n",
    "    losses.append(current_loss)\n",
    "    if epoch % 100 == 0:\n",
    "        pbar.set_postfix(loss=current_loss)\n",
    "    if current_loss < 0.001:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finally, we plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with two subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 2))\n",
    "\n",
    "# Plot the loss over the number of iterations\n",
    "axs[0].set_yscale('log')\n",
    "axs[0].plot(range(len(losses)), losses, label='Loss')\n",
    "axs[0].set_xlabel('Iteration')\n",
    "axs[0].set_ylabel('Loss')\n",
    "axs[0].legend()\n",
    "\n",
    "# Plot the results\n",
    "predictions = vmap(nn.forward, in_axes=(None, 0))(nn.params, x)\n",
    "axs[1].plot(x, y, label='Sine function')\n",
    "axs[1].plot(x, jnp.squeeze(predictions), label='Neural network')\n",
    "axs[1].legend()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Domain Decomposition for Physics-Informed Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we apply the decomposition approach from the previous section in the context of physics-informed neural networks. Therefore, consider the ordinary differential equation\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "        u'(x) & = k \\pi \\cos(k \\pi x), \\quad x \\in [-1,1], \\\\\n",
    "        u(0) & = 0.\n",
    "    \\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Then, the exact solution is given by\n",
    "$$\n",
    "    u(x) = \\sin(k \\pi x).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Python Example of Domain Decomposition for Physics-Informed Neural Networks Without Domain Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first try to approximate the solution using a singe physics-informed neural network without domain decomposition. In particular, we employ a simple neural network with a single hidden layer $\\mathcal{N}_\\theta (x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For simplicity, we enforce the boundary condition $u(0) = 0$ via hard constraints:\n",
    "$$\n",
    "    u_{\\theta} (x) = \\tanh(k \\pi x) N_\\theta (x)\n",
    "$$\n",
    "\n",
    "Therefore, in total, we employ the physics-informed loss function\n",
    "$$\n",
    "    \\mathcal{L}(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} \\left\\| {u_{\\theta}}' (x_i) - k \\pi \\cos(k \\pi x_i) \\right\\|^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from jax import grad\n",
    "\n",
    "# Generate sample data\n",
    "k = 16\n",
    "x = jnp.linspace(-1.0, 1.0, 1000)\n",
    "y = jnp.sin(k * jnp.pi * x)\n",
    "\n",
    "# Initialize the domain decomposed neural network\n",
    "key = random.PRNGKey(0)\n",
    "layer_sizes = [1, 160, 1]\n",
    "nn = FeedForwardNN(layer_sizes, key)\n",
    "\n",
    "# Define the neural network function u\n",
    "sigma = jit(lambda x: jax.nn.tanh(k * jnp.pi * x))\n",
    "u_nn = jit(lambda params, x: sigma(x) * nn.forward(params, x).squeeze())\n",
    "u_nn_batch = vmap(u_nn, (None, 0))\n",
    "\n",
    "# Compute gradients\n",
    "u_nn_x = grad(u_nn, argnums=1)\n",
    "\n",
    "# Define the residual function\n",
    "squared_residual = jit(lambda params, x: (u_nn_x(params, x) - k * jnp.pi * jnp.cos(k * jnp.pi * x)) ** 2)\n",
    "squared_residual_batch = vmap(squared_residual, (None, 0))\n",
    "\n",
    "# Define the physics-informed loss function\n",
    "@jit\n",
    "def physics_informed_loss(params, x):\n",
    "    physics_loss = jnp.mean(squared_residual_batch(params, x))\n",
    "    return physics_loss\n",
    "\n",
    "# Define training step with Adam optimizer\n",
    "def train_step(params, opt_state, x):\n",
    "    loss, grads = jax.value_and_grad(physics_informed_loss)(params, x)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return new_params, opt_state, loss\n",
    "\n",
    "# Create a learning rate schedule\n",
    "learning_rate_schedule = optax.piecewise_constant_schedule(\n",
    "    init_value=0.01,\n",
    "    # boundaries_and_scales={200: 0.1}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We set up the training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Adam optimizer with the learning rate schedule\n",
    "optimizer = optax.adam(learning_rate=learning_rate_schedule)\n",
    "opt_state = optimizer.init(nn.params)\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "max_iterations = 20000\n",
    "\n",
    "x = jnp.linspace(-1.0, 1.0, 1000)\n",
    "\n",
    "# Training loop with tqdm progress bar\n",
    "pbar = trange(max_iterations, desc=\"Training\", leave=True)\n",
    "for epoch in pbar:\n",
    "    nn.params, opt_state, current_loss = train_step(nn.params, opt_state, x)\n",
    "    losses.append(current_loss)\n",
    "    if epoch % 100 == 0:\n",
    "        pbar.set_postfix(loss=current_loss)\n",
    "    if current_loss < 0.001:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And finally, we plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with two subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 2))\n",
    "\n",
    "# Plot the loss over the number of iterations\n",
    "axs[0].set_yscale('log')\n",
    "axs[0].plot(range(max_iterations), losses, label='Loss')\n",
    "axs[0].set_xlabel('Iteration')\n",
    "axs[0].set_ylabel('Loss')\n",
    "axs[0].legend()\n",
    "\n",
    "# Plot the results\n",
    "predictions = u_nn_batch(nn.params, x)\n",
    "axs[1].plot(x, y, label='Sine function')\n",
    "axs[1].plot(x, jnp.squeeze(predictions), label='Neural network')\n",
    "axs[1].set_ylim([-1, 1])\n",
    "axs[1].legend()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### First Python Example of Domain Decomposition for Physics-Informed Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combining the the domain decomposition approach with physics-informed neural networks**, we decompose the interval $[-1,1]$ into $k$ subintervals $I_1,\\ldots,I_k$ and use the same acticture as before:\n",
    "$$\n",
    "    \\mathcal{N}_{\\theta} (x) = \\sum_{i=j}^{k} \\omega_j (x) \\mathcal{N}_{j,\\theta_j} ({\\rm norm}(x)),\n",
    "$$\n",
    "with\n",
    "$$\n",
    "    \\omega_j \n",
    "    =\n",
    "    \\begin{cases}\n",
    "        1 & \\text{if} \\quad x \\in I_j, \\\\\n",
    "        0 & \\text{otherwise}.\n",
    "    \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "With strong enforcement of boundary constraints, we obtain\n",
    "$$\n",
    "    u_{\\theta} (x) = \\tanh(k \\pi x) \\sum_{i=j}^{k} \\omega_j (x) \\mathcal{N}_{j,\\theta_j} ({\\rm norm}(x))\n",
    "$$\n",
    "and the same physics-informed loss function\n",
    "$$\n",
    "    \\mathcal{L}(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} \\left\\| {u_{\\theta}}' (x_i) - k \\pi \\cos(k \\pi x_i) \\right\\|^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We set up the loss function and training step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import jit, grad\n",
    "\n",
    "# Generate sample data\n",
    "k = 16\n",
    "x = jnp.linspace(-1.0, 1.0, 1000)\n",
    "y = jnp.sin(k * jnp.pi * x)\n",
    "\n",
    "# Initialize the domain decomposed neural network\n",
    "key = random.PRNGKey(0)\n",
    "layer_sizes = [1, 10, 1]\n",
    "nn = DomainDecomposedNN(k, layer_sizes, key)\n",
    "\n",
    "# Define the neural network function u\n",
    "sigma = jit(lambda x: jax.nn.tanh(k * jnp.pi * x))\n",
    "u_nn = jit(lambda params, x: sigma(x) * nn.forward(params, x))\n",
    "u_nn_batch = vmap(u_nn, (None, 0))\n",
    "\n",
    "# Compute gradients\n",
    "u_nn_x = grad(u_nn, argnums=1)\n",
    "\n",
    "# Define the residual function\n",
    "squared_residual = jit(lambda params, x: (u_nn_x(params, x) - k * jnp.pi * jnp.cos(k * jnp.pi * x)) ** 2)\n",
    "squared_residual_batch = vmap(squared_residual, (None, 0))\n",
    "\n",
    "# Define the physics-informed loss function\n",
    "@jit\n",
    "def physics_informed_loss(params, x):\n",
    "    physics_loss = jnp.mean(squared_residual_batch(params, x))\n",
    "    return physics_loss\n",
    "\n",
    "# Define training step with Adam optimizer\n",
    "def train_step(params, opt_state, x):\n",
    "    loss, grads = jax.value_and_grad(physics_informed_loss)(params, x)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return new_params, opt_state, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a learning rate schedule\n",
    "learning_rate_schedule = optax.piecewise_constant_schedule(\n",
    "    init_value=0.01,\n",
    "    # boundaries_and_scales={200: 0.1}\n",
    ")\n",
    "\n",
    "# Initialize the Adam optimizer with the learning rate schedule\n",
    "optimizer = optax.adam(learning_rate=learning_rate_schedule)\n",
    "opt_state = optimizer.init(nn.params)\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "max_iterations = 5000\n",
    "\n",
    "x = jnp.linspace(-1.0, 1.0, 1000)\n",
    "\n",
    "# Training loop with tqdm progress bar\n",
    "pbar = trange(max_iterations, desc=\"Training\", leave=True)\n",
    "for epoch in pbar:\n",
    "    nn.params, opt_state, current_loss = train_step(nn.params, opt_state, x)\n",
    "    losses.append(current_loss)\n",
    "    if epoch % 100 == 0:\n",
    "        pbar.set_postfix(loss=current_loss)\n",
    "    if current_loss < 0.001:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finally, we plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create a figure with two subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 2))\n",
    "\n",
    "# Plot the loss over the number of iterations\n",
    "axs[0].set_yscale('log')\n",
    "axs[0].plot(range(max_iterations), losses, label='Loss')\n",
    "axs[0].set_xlabel('Iteration')\n",
    "axs[0].set_ylabel('Loss')\n",
    "axs[0].legend()\n",
    "\n",
    "# Plot the results\n",
    "predictions = u_nn_batch(nn.params, x)\n",
    "axs[1].plot(x, y, label='Sine function')\n",
    "axs[1].plot(x, jnp.squeeze(predictions), label='Neural network')\n",
    "axs[1].set_ylim([-1, 1])\n",
    "axs[1].legend()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Somehow, this does not seem to work properly. Recalling the Schwarz methods, the **overlap was essential for transferring information between the subdomains**. In particular, the neural network function\n",
    "$$\n",
    "    \\mathcal{N}_{\\theta} (x) = \\sum_{i=j}^{k} \\omega_j (x) \\mathcal{N}_{j,\\theta_j} ({\\rm norm}(x)),\n",
    "$$\n",
    "is **not necessarily continuous** accross the whole domain $[-1,1]$ because at the interface $\\hat x_{j+1} = I_j \\cap I_{j+1}$, we can have that\n",
    "$$\n",
    "    \\lim_{x \\nearrow \\hat x_{j+1}^-} \\omega_j (x) \\mathcal{N}_{j,\\theta_j} ({\\rm norm}(x)) \\neq \\lim_{x \\searrow \\hat x_{j+1}^+} \\omega_j (x) \\mathcal{N}_{j+1,\\theta_{j+1}} ({\\rm norm}(x)),\n",
    "$$\n",
    "and there is no constaint on the neural network functions $\\mathcal{N}_{j,\\theta_j}$ and $\\mathcal{N}_{j+1,\\theta_{j+1}}$ to balance that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In particular, the differential equation\n",
    "$$\n",
    "    u'(x) = k \\pi \\cos(k \\pi x),\n",
    "$$\n",
    "and hence the physics loss, only constrains the derivative of the neural network function. This is not the case for function approximation, where the loss constrains the function values itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Overlapping Schwarz Domain Decomposition for Physics-Informed Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the final part of today's lecture, we will show that using an **overlapping domain decomposition can solve this problem**. Therefore, we will now employ a neural network architecture which is based on an overlapping domain decomposition, which also requires us to modify the partition of unity functions $\\omega_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is based on the papers:\n",
    "> Moseley, B., Markham, A., & Nissen-Meyer, T. (2023). Finite basis physics-informed neural networks (FBPINNs): a scalable domain decomposition approach for solving differential equations. Advances in Computational Mathematics, 49(4), 62.\n",
    "\n",
    "> Dolean, V., Heinlein, A., Mishra, S., & Moseley, B. (2024). Multilevel domain decomposition-based architectures for physics-informed neural networks. Computer Methods in Applied Mechanics and Engineering, 429, 117116."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let us first decompose the interval $[-1,1]$ into $k$ overlapping subintervals $I_1, \\ldots, I_k$, with \n",
    "$$\n",
    "    I_j = \\big[ \\underbrace{\\frac{2(j-1) - \\delta}{k}-1}_{x_j^s}, \\underbrace{\\frac{2j+\\delta}{k}-1}_{x_j^e} \\big] \\cap [-1,1].\n",
    "$$\n",
    "where $\\delta$ indicates the relative overlap of the subdomains. In order to obtain a partition of unity on these overlapping intervals, we define the partition of unity functions as follows\n",
    "$$\n",
    "    \\omega_j (x) \n",
    "    =\n",
    "    \\begin{cases}\n",
    "        0 & \\text{if} \\quad x < x_j^s, \\\\\n",
    "        \\frac{1}{2}\\left(1-\\cos\\left(k \\pi \\frac{x - x_j^s}{2\\delta}\\right)\\right) & \\text{if} \\quad x_j^s \\leq x < x_j^s+\\frac{2\\delta}{k} \\in I_j, \\\\\n",
    "        1 & \\text{if} \\quad x_j^s+\\frac{2\\delta}{k} \\leq x < x_j^e-\\frac{2\\delta}{k}, \\\\\n",
    "        \\frac{1}{2}\\left(1+\\cos\\left(k \\pi \\frac{x - x_j^e}{2\\delta} + \\pi \\right)\\right) & \\text{if} \\quad x_j^e-\\frac{2\\delta}{k} \\leq x < x_j^e \\in I_j, \\\\\n",
    "        0 & \\text{if} \\quad x_j^e \\leq x.\n",
    "    \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This function looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parameters\n",
    "k = 16\n",
    "delta = 0.5\n",
    "j = 2\n",
    "\n",
    "def omega_j(x, j, k, delta):\n",
    "    if j == 0:\n",
    "        x_js = -1 - 2 * delta\n",
    "    else :\n",
    "        x_js = (2 * j - delta) / k - 1\n",
    "    if j == k-1:\n",
    "        x_je = 1 + 2 * delta\n",
    "    else :\n",
    "        x_je = (2 * (j + 1) + delta) / k - 1\n",
    "    return jnp.where(\n",
    "        (x < x_js) | (x >= x_je), 0.0,\n",
    "        jnp.where(\n",
    "            (x_js <= x) & (x < x_js + 2 * delta / k),\n",
    "            0.5*(1 - jnp.cos(jnp.pi * k * (x - x_js) / (2 * delta))),\n",
    "            jnp.where(\n",
    "                (x_js + 2 * delta / k <= x) & (x < x_je - 2 * delta / k),\n",
    "                1.0,\n",
    "                jnp.where(\n",
    "                    (x_je - 2 * delta / k <= x) & (x < x_je),\n",
    "                    0.5*(1+jnp.cos(jnp.pi * k * (x - x_je) / (2 * delta) + jnp.pi)),\n",
    "                    0.0\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Generate x values\n",
    "x = jnp.linspace(-1.0, 1.0, 1000)\n",
    "omega_values = omega_j(x, j, k, delta)\n",
    "\n",
    "# Compute the gradient of omega_j\n",
    "omega_grad = grad(omega_j, argnums=0)\n",
    "omega_grad_batch = vmap(omega_grad, in_axes=(0, None, None, None))\n",
    "omega_grad_values = omega_grad_batch(x, j, k, delta)\n",
    "\n",
    "# Plot the function and its gradient\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel(f'omega_{j}(x)', color='tab:blue')\n",
    "ax1.plot(x, omega_values, label=f'omega_{j}(x)', color='tab:blue')\n",
    "ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel(f\"omega_{j}'(x)\", color='tab:red')\n",
    "ax2.plot(x, omega_grad_values, label=f\"omega_{j}'(x)\", color='tab:red')\n",
    "ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.title(f'Plot of omega_{j}(x) and its gradient with j={j}, k={k}, delta={delta}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let us now set up the overlapping domain decomposition-based neural network architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the domain decomposition neural network\n",
    "class OverlappingDomainDecomposedNN:\n",
    "    def __init__(self, num_subdomains, overlap, layer_sizes, key, activation_fn=jax.nn.tanh):\n",
    "        self.num_subdomains = num_subdomains\n",
    "        self.overlap = overlap\n",
    "        self.subnets = [FeedForwardNN(layer_sizes, key, activation_fn) for j in range(num_subdomains)]\n",
    "        self.params = [subnet.params for subnet in self.subnets]\n",
    "        self.x_js = jnp.array([jnp.maximum((2 * j - overlap) / num_subdomains - 1.0, -1.0) for j in range(num_subdomains)])\n",
    "        self.x_je = jnp.array([jnp.minimum((2 * (j + 1) + overlap) / num_subdomains - 1.0, 1.0) for j in range(num_subdomains)])\n",
    "\n",
    "    def forward(self, params, x):\n",
    "        outputs = []\n",
    "        for i in range(self.num_subdomains):\n",
    "            mask = omega_j(x, i, self.num_subdomains, self.overlap)\n",
    "            norm_x = 2 * (x - (self.x_js[i] + self.x_je[i]) / 2) / (self.x_je[i] - self.x_js[i])\n",
    "            outputs.append(mask * self.subnets[i].forward(params[i], norm_x))\n",
    "        return jnp.sum(jnp.array(outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Then, we set up the neural network and loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import jit, grad\n",
    "\n",
    "# Parameters\n",
    "k = 16\n",
    "delta = 0.5\n",
    "\n",
    "# Generate sample data\n",
    "x = jnp.linspace(-1.0, 1.0, 1000)\n",
    "y = jnp.sin(k * jnp.pi * x)\n",
    "\n",
    "# Initialize the domain decomposed neural network\n",
    "key = random.PRNGKey(0)\n",
    "layer_sizes = [1, 20, 1]\n",
    "nn = OverlappingDomainDecomposedNN(k, delta, layer_sizes, key)\n",
    "\n",
    "# Define the neural network function u\n",
    "sigma = jit(lambda x: jax.nn.tanh(k * jnp.pi * x))\n",
    "u_nn = jit(lambda params, x: nn.forward(params, x) * sigma(x))\n",
    "u_nn_batch = vmap(u_nn, (None, 0))\n",
    "\n",
    "# Compute gradients\n",
    "u_nn_x = grad(u_nn, argnums=1)\n",
    "\n",
    "# Define the residual function\n",
    "squared_residual = jit(lambda params, x: (u_nn_x(params, x) - k * jnp.pi * jnp.cos(k * jnp.pi * x)) ** 2)\n",
    "squared_residual_batch = vmap(squared_residual, (None, 0))\n",
    "\n",
    "# Define the physics-informed loss function\n",
    "@jit\n",
    "def physics_informed_loss(params, x):\n",
    "    physics_loss = jnp.mean(squared_residual_batch(params, x))\n",
    "    return physics_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Then, we train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training step with Adam optimizer\n",
    "def train_step(params, opt_state, x):\n",
    "    loss, grads = jax.value_and_grad(physics_informed_loss)(params, x)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return new_params, opt_state, loss\n",
    "\n",
    "# Create a learning rate schedule\n",
    "learning_rate_schedule = optax.piecewise_constant_schedule(\n",
    "    init_value=0.01,\n",
    "    boundaries_and_scales={2500: 0.1}\n",
    ")\n",
    "\n",
    "# Initialize the Adam optimizer with the learning rate schedule\n",
    "optimizer = optax.adam(learning_rate=learning_rate_schedule)\n",
    "opt_state = optimizer.init(nn.params)\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "max_iterations = 2000\n",
    "\n",
    "# Training loop with tqdm progress bar\n",
    "pbar = trange(max_iterations, desc=\"Training\", leave=True)\n",
    "for epoch in pbar:\n",
    "    nn.params, opt_state, current_loss = train_step(nn.params, opt_state, x)\n",
    "    losses.append(current_loss)\n",
    "    if epoch % 100 == 0:\n",
    "        pbar.set_postfix(loss=current_loss)\n",
    "    if current_loss < 0.001:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finally, we plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with two subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 2))\n",
    "\n",
    "# Plot the loss over the number of iterations\n",
    "axs[0].set_yscale('log')\n",
    "axs[0].plot(range(max_iterations), losses, label='Loss')\n",
    "axs[0].set_xlabel('Iteration')\n",
    "axs[0].set_ylabel('Loss')\n",
    "axs[0].legend()\n",
    "\n",
    "# Plot the results\n",
    "predictions = u_nn_batch(nn.params, x)\n",
    "axs[1].plot(x, y, label='Sine function')\n",
    "axs[1].plot(x, predictions, label='Neural network')\n",
    "axs[1].set_ylim([-1, 1])\n",
    "axs[1].legend()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Some Final Remarks on the importance of the overlap\n",
    "\n",
    "As an **alternative to using an overlapping domain decomposition**, we can also modify the approach of a nonoverlapping domain decomposition. As mentioned before, the problem is the missing transfer of spatial information provided by the overlap. To account for that, we can alternatively enforce continuity of the solution accross the whole domain $[-1,1]$ via additional constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In particular, we could enforce \n",
    "$$\n",
    "    \\lim_{x \\nearrow \\hat x_{j+1}^-} \\omega_j (x) \\mathcal{N}_{j,\\theta_j} ({\\rm norm}(x)) \\neq \\lim_{x \\searrow \\hat x_{j+1}^+} \\omega_j (x) \\mathcal{N}_{j+1,\\theta_{j+1}} ({\\rm norm}(x)),\n",
    "$$\n",
    "by adding the constraint to the loss function:\n",
    "$$\n",
    "    \\left( \\omega_j (x) \\mathcal{N}_{j,\\theta_j} ({\\rm norm}(x_i)) - \\omega_{j+1} (x) \\mathcal{N}_{j+1,\\theta_{j+1}} ({\\rm norm}(x_i)) \\right)^2.\n",
    "$$\n",
    "for all \n",
    "$$\n",
    "    x \\in \\bigcup_{j = 1,\\ldots,k-1} ( I_j \\cap I_{j+1} ). \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Thank you for your attention!\n",
    "\n",
    "## Questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D Laplace Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "k = 8\n",
    "delta = 0.5\n",
    "\n",
    "# Generate sample data\n",
    "x = jnp.linspace(0.0, 1.0, 1000)\n",
    "y = 0.5 * x * (1 - x)\n",
    "\n",
    "# Initialize the domain decomposed neural network\n",
    "key = random.PRNGKey(0)\n",
    "layer_sizes = [1, 20, 1]\n",
    "nn = OverlappingDomainDecomposedNN(k, delta, layer_sizes, key)\n",
    "\n",
    "# Define the neural network function u\n",
    "sigma = jit(lambda x: x * (1 - x))\n",
    "u_nn = jit(lambda params, x: sigma(x) * nn.forward(params, x).squeeze())\n",
    "u_nn_batch = vmap(u_nn, (None, 0))\n",
    "\n",
    "# Compute gradients\n",
    "u_nn_x = grad(u_nn, argnums=1)\n",
    "u_nn_xx = grad(u_nn_x, argnums=1)\n",
    "\n",
    "# Define the residual function\n",
    "squared_residual = jit(lambda params, x: (u_nn_xx(params, x) + 1.0) ** 2)\n",
    "squared_residual_batch = vmap(squared_residual, (None, 0))\n",
    "\n",
    "# Define the physics-informed loss function\n",
    "@jit\n",
    "def physics_informed_loss(params, x):\n",
    "    physics_loss = jnp.mean(squared_residual_batch(params, x))\n",
    "    return physics_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training step with Adam optimizer\n",
    "def train_step(params, opt_state, x):\n",
    "    loss, grads = jax.value_and_grad(physics_informed_loss)(params, x)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return new_params, opt_state, loss\n",
    "\n",
    "# Create a learning rate schedule\n",
    "learning_rate_schedule = optax.piecewise_constant_schedule(\n",
    "    init_value=0.1,\n",
    "    boundaries_and_scales={2500: 0.1}\n",
    ")\n",
    "\n",
    "# Initialize the Adam optimizer with the learning rate schedule\n",
    "optimizer = optax.adam(learning_rate=learning_rate_schedule)\n",
    "opt_state = optimizer.init(nn.params)\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "max_iterations = 2000\n",
    "\n",
    "# Training loop with tqdm progress bar\n",
    "pbar = trange(max_iterations, desc=\"Training\", leave=True)\n",
    "for epoch in pbar:\n",
    "    nn.params, opt_state, current_loss = train_step(nn.params, opt_state, x)\n",
    "    losses.append(current_loss)\n",
    "    if epoch % 100 == 0:\n",
    "        pbar.set_postfix(loss=current_loss)\n",
    "    if current_loss < 0.0001:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with two subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 2))\n",
    "\n",
    "# Plot the loss over the number of iterations\n",
    "axs[0].set_yscale('log')\n",
    "axs[0].plot(range(len(losses)), losses, label='Loss')\n",
    "axs[0].set_xlabel('Iteration')\n",
    "axs[0].set_ylabel('Loss')\n",
    "axs[0].legend()\n",
    "\n",
    "# Plot the results\n",
    "predictions = u_nn_batch(nn.params, x)\n",
    "axs[1].plot(x, y, label='Sine function')\n",
    "axs[1].plot(x, predictions, label='Neural network')\n",
    "axs[1].set_xlim([0, 1.0])\n",
    "axs[1].set_ylim([0, 0.5])\n",
    "axs[1].legend()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
